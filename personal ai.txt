Below is the Updated Product Requirements Document (PRD), incorporating your new requirements:

Product Requirements Document (PRD)
Local AI Agent with Enhanced Learning and Testing
1. Introduction
This PRD describes a local AI Agent running on an Apple M4 Max laptop. The agent leverages local LLMs via Ollama, integrates with a vector database (ChromaDB) for long-term memory, supports document ingestion (PDF, DOCX, TXT, CSV, JSON), and features internet search/web scraping (including a BabyAGI-like framework for autonomous research). It also includes an internal Python code execution mechanism for on-demand computations.
New Additional Requirements:
The AI agent learns and expands its domain knowledge over time by referencing stored memories and past chat contexts in all future replies.
A comprehensive testing suite that covers the entire codebase, ensuring that any errors or bugs found during tests are thoroughly analyzed and explained. This approach will enable other AI systems or developers to interpret the test logs and propose targeted fixes or enhancements.

2. Core Capabilities
2.1 Long-Term Memory (ChromaDB)
Persistent Context: Store every conversation, user message, AI response, web-scraped data, and document embeddings in ChromaDB.
Contextual Recall: Retrieve and embed previous conversation snippets or relevant data at query time.
Cumulative Understanding: The AI agent not only references past chats but also expands its domain knowledge. Each new piece of information (from user prompts, web searches, or document uploads) enriches its memory vector store, allowing deeper and more coherent responses over time.
Requirement: The AI must regularly review or summarize relevant historical context from ChromaDB to improve its coherence and maintain an internal “state of knowledge” that is progressively updated.
2.2 Internet Search & Web Scraping (BabyAGI)
Automated Research: Use a BabyAGI-like loop to break down complex tasks into subtasks, search for relevant data, and scrape websites autonomously.
Memory Ingestion: All discovered knowledge is parsed, summarized, embedded, and stored in ChromaDB, augmenting the AI’s knowledge base.
Task Prioritization: Optionally, the agent can reorder tasks based on priority, resource constraints, or intermediate results.
2.3 Document Uploading & Indexing
Supported Formats: PDF, DOCX, TXT, CSV, JSON.
Chunking & Embedding: Automatically parse, chunk, embed, and store text in ChromaDB for efficient retrieval.
On-Demand Q&A: Retrieve relevant chunks at query time to answer user questions grounded in the uploaded documents.
2.4 Internal Python Code Execution
Safe Sandbox: An embedded Python interpreter (or subprocess) that runs user-supplied or AI-generated Python code safely and returns outputs or errors.
Error Handling: Catch all exceptions and present them to the user in a friendly, interpretable manner.
Extended Reasoning: The AI can use Python to validate arithmetic, run simulations, or handle quick data transformations that require code.
2.5 Dynamic LLM Selection (Ollama)
Multiple Models: Integrate small, medium, and large local LLMs for different tasks.
Rules-based or Heuristic-based: Decide which model to invoke, balancing speed vs. accuracy.
Prompt Format Adapters: Each model’s prompt style is abstracted so that the agent can switch models seamlessly.

3. Enhanced Learning & Domain Expansion
3.1 Continuous Context Building
Context Linking: The agent links newly acquired knowledge with existing memory entries, forming richer embeddings for future reference.
Retrospective Summaries: Periodically generate “summaries” of older or thematically related interactions to keep the “active” memory concise but still accessible.
Adaptive Conversations: Before replying, the AI will:
Identify relevant past messages/documents.
Incorporate them into a structured prompt.
Generate the response that explicitly references or factors in this historical context.
3.2 Deepening Domain Knowledge
If the user repeatedly discusses a certain domain (e.g., finance, medicine, programming), the agent’s memory embeddings in that domain grow richer.
Over time, the AI’s knowledge appears more specialized, as it references increasingly detailed or relevant past contexts in its answers.

4. Implementation Details
4.1 Lean Python Codebase
Modular Organization:
agent.py – orchestrates conversation flow, handles prompt construction.
memory.py – manages ChromaDB interactions.
doc_index.py – parses, chunks, and indexes uploaded documents.
search.py – web search and scraping logic with BabyAGI-like task loop.
executor.py – code execution in a secure sandbox.
cli.py – CLI user interface and command handling.
tests/ – entire test suite.
Minimal Dependencies: Only essential libraries for vector DB, doc parsing, web scraping, etc.
4.2 ChromaDB Integration
Data Model: Each embedded chunk = (embedding, metadata, text_id).
Collections:
conversations (for user + AI messages)
web_data (for scraped content)
docs (for uploaded documents)
Retrieval: By similarity search on embeddings, combined with metadata filters if needed.
Periodic Summaries: Summaries stored back into the DB to reduce token overhead.
4.3 Enhanced AI Reasoning Flow
When the user prompts the agent:
Gather Context: Query ChromaDB for relevant conversation or document snippets.
Assemble Prompt: Formulate a “system/context prompt” referencing the found information (and possibly summarizing it).
Model Selection: Choose an appropriate LLM from Ollama (small, medium, or large).
Generate Answer: LLM produces the response.
Memory Update: Store the entire conversation turn + new context in ChromaDB.

5. Testing & Validation
5.1 Comprehensive Test Suite (Pytest)
5.1.1 Unit Tests
Memory Tests: Verify insertion, retrieval, updates in ChromaDB.
Document Parsing Tests: For each file format, confirm text extraction correctness.
Web Search & Scraping Tests: Mock external calls. Validate the agent extracts data properly.
Code Execution Tests: Test successful executions (e.g., print('Hello')) and error handling (e.g., 1/0).
5.1.2 Integration Tests
Conversation Flow: End-to-end tests simulate user input -> agent response -> memory usage.
AI Summaries: Check that old messages are correctly summarized and referenced.
BabyAGI Loop: Validate multi-step tasks are created, prioritized, executed, and results stored.
5.1.3 Enhanced Domain Knowledge Tests
Repeated Domain Queries: Interact with the agent multiple times on the same topic. Check if it accumulates relevant info.
Cross-References: Insert knowledge across multiple documents and web data, then ask a composite question that requires pulling from both. Confirm correct retrieval.
5.1.4 Error & Bug Tests
Resilience: Induce known errors (e.g., file not found, malformed PDF, network time-out) and verify graceful handling.
Logging & Explanation: Confirm that test logs and error messages are thorough enough for an external system (like another AI or developer) to parse and interpret.
5.2 Detailed Error Analysis
Every test or sub-test that fails should produce:
Error Type: e.g., FileNotFoundError or custom exceptions.
Stack Trace: Precisely where the error occurred in the code.
Cause Explanation: The test framework (Pytest plugins or custom logging) should present a plain-English explanation of why the error might have happened (e.g., “The file path is incorrect or the file does not exist in the directory.”).
Potential Fixes: If feasible, suggest common solutions (e.g., “Ensure the file path is correct. Check if you have read permissions.”).
Rationale: This level of detail allows other LLMs or human developers to quickly parse the test logs and propose targeted fixes. For example, a GPT-based system reading these logs can then say, “The document parsing function needs an additional check for file type before attempting extraction.”
5.3 Reporting for AI LLM Consumption
Structured Log Format: Possibly in JSON or a standardized format.
Example:
{
  "test_name": "test_parse_pdf_valid",
  "status": "fail",
  "error_type": "FileNotFoundError",
  "explanation": "File path not found at '/docs/missing.pdf'.",
  "suggested_fix": "Check if '/docs/missing.pdf' exists or if path is correct."
}

Summaries: The final test report is aggregated, giving an overview of pass/fail counts, error types, and a short summary for each.
Post-Run Analysis: Optionally, the system can produce a single “report.md” file with a human-readable explanation of all errors, test coverage stats, and recommendations for improvement.

6. Documentation
6.1 111,111 Token Summarization
After implementation, generate an extensive (up to 111,111 tokens) summary of:
Code Structure: Each module, function, class, how they interact.
Frameworks: Explanation of ChromaDB, BabyAGI pattern, Ollama usage.
Execution Flow: Lifecycle of a user query, from memory retrieval to final answer.
Test Results: Summaries of all test cases, any errors, and solutions. This large summary can be fed into an LLM to facilitate deeper analysis or improvement suggestions.
6.2 Beginner-Friendly README
Setup Guide: Step-by-step (e.g., pip install -r requirements.txt, how to install Ollama, etc.).
Usage: How to run the CLI, load documents, do web searches, or run code.
Reasoning: For each major step (like installing ChromaDB), explain why it’s needed.
Troubleshooting: Common pitfalls, how to interpret test results, and how to fix errors.

7. Conclusion
This document specifies a robust, extensible local AI Agent with expanded memory-based domain knowledge and a comprehensive testing suite. The agent continually learns from user interactions, documents, and web data, storing all context in ChromaDB for recall. The testing framework not only detects bugs and errors but also provides detailed analysis suitable for AI-based or human debugging.
By following these requirements, a junior Python developer (and subsequent AI systems) can implement, maintain, and continuously improve the AI agent with minimal confusion or friction. The final result is a powerful, privacy-first conversational assistant that grows more capable the more it’s used—backed by thorough testing and documentation.
